{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399fe49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "plt.rcParams['font.family'] = 'Bookman Old Style'\n",
    "\n",
    "\n",
    "class dataset:\n",
    "    def __init__(self, x, y, num_training_points=5000, num_memory_levels =3):\n",
    "        self.num_training_points = num_training_points\n",
    "        self.num_memory_levels = num_memory_levels # If there are 0 memory taps, this is 1\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        self.model_training_input, self.model_training_output, self.training_phase, self.model_valid_input, self.model_valid_output, self.valid_phase = self.prepare_data()\n",
    "\n",
    "    def phase_vector(self, x):    \n",
    "        \"\"\"Takes a vector x and returns a vector of phases of each element\"\"\"\n",
    "        Ax = np.abs(x)\n",
    "        return np.conj(x)/Ax\n",
    "\n",
    "    def model_expected_output(self, y, phase):\n",
    "        \"\"\"Take in data, phase normalised it, and trim. Return as IQ seperately\"\"\"\n",
    "        y_denorm = y*phase\n",
    "        y_denorm_trim = y_denorm[self.num_memory_levels:]\n",
    "        return np.array([np.real(y_denorm_trim), np.imag(y_denorm_trim)]).T\n",
    "\n",
    "    def build_xfc(self, x, num_memory_levels):\n",
    "        \"\"\"\n",
    "        Replicates the MATLAB build_xfc() function.\n",
    "        \"\"\"\n",
    "        num_points = len(x)\n",
    "        phase = self.phase_vector(x)\n",
    "        I = np.real(x)\n",
    "        Q = np.imag(x)\n",
    "\n",
    "        # Phase-normalized data\n",
    "        phase_norm_data = np.zeros((num_points, num_memory_levels), dtype=complex)\n",
    "        for n in range(num_memory_levels, num_points):\n",
    "            for m in range(num_memory_levels):\n",
    "                phase_norm_data[n, m] = x[n - m - 1] * phase[n]\n",
    "\n",
    "        # Ax magnitude feature\n",
    "        Ax = np.sqrt(I**2 + Q**2)\n",
    "\n",
    "        # Build A feature matrix (Ax memory taps)\n",
    "        A_feats = np.zeros((num_points, num_memory_levels))\n",
    "        for n in range(num_memory_levels, num_points):\n",
    "            for m in range(num_memory_levels):\n",
    "                A_feats[n, m] = Ax[n - m]\n",
    "\n",
    "        # Trim first num_memory_levels samples (as in MATLAB)\n",
    "        phase_norm_data = phase_norm_data[num_memory_levels:, :]\n",
    "        A_feats = A_feats[num_memory_levels:, :]\n",
    "        A3_feats = A_feats ** 3\n",
    "\n",
    "        # Combine real and imaginary phase-normalized parts with A-features\n",
    "        xfc = np.hstack([\n",
    "            np.real(phase_norm_data),\n",
    "            np.imag(phase_norm_data),\n",
    "            A_feats,\n",
    "            A3_feats\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        return xfc\n",
    "\n",
    "    def build_x_matrix(self, x, num_mem_levels, num_nl_orders):\n",
    "        \"\"\"Build Matrix X for find Volterra Model\"\"\"\n",
    "        num_points = len(x)\n",
    "        X = np.zeros((num_points, num_mem_levels * num_nl_orders), dtype=np.complex128)\n",
    "        \n",
    "        for n in range(num_mem_levels - 1, num_points):\n",
    "            col = 0\n",
    "            for i in range(num_mem_levels):\n",
    "                xi = x[n - i]\n",
    "                for j in range(num_nl_orders):\n",
    "                    X[n, col] = (abs(xi) ** ((j) * 2)) * xi\n",
    "                    col += 1\n",
    "\n",
    "        return X\n",
    "\n",
    "    def build_y(self, u, A, num_mem_levels, num_nl_orders):\n",
    "        \"\"\"Builds y, the output of the volterra Model. Trims Output\"\"\"\n",
    "        num_points = len(u)\n",
    "        y = np.zeros((num_points, 1), dtype=np.complex128)\n",
    "        for n in range(num_mem_levels - 1, num_points):\n",
    "            col = 0 \n",
    "            for i in range(num_mem_levels):\n",
    "                ui = u[n-i]\n",
    "                for j in range(num_nl_orders):\n",
    "                    y[n]= y[n] + A[col]*(abs(ui)**(j*2)*ui)\n",
    "                    col += 1\n",
    "        y = y[self.num_memory_levels:]\n",
    "        return y\n",
    "            \n",
    "\n",
    "    def volterra(self,num_nl_orders):\n",
    "        \"\"\"Build component matrix A\"\"\"\n",
    "        X = self.build_x_matrix(self.model_training_input, self.num_memory_levels, num_nl_orders)\n",
    "        \n",
    "        X_trim = X[self.num_memory_levels:, :]\n",
    "        y_trim = self.model_training_output[self.num_memory_levels:]\n",
    "        return np.linalg.pinv(X_trim.conj().T @ X_trim) @ (X_trim.conj().T @ y_trim);\n",
    "\n",
    "    def training_data(self):\n",
    "        \"\"\"Assign some data for just training\"\"\"\n",
    "        idx_training = range(0, self.num_training_points -1) # training indices\n",
    "\n",
    "        model_training_input = self.y[idx_training]\n",
    "        model_training_output = self.x[idx_training]\n",
    "        return model_training_input, model_training_output\n",
    "    \n",
    "\n",
    "    def validation_data(self):\n",
    "        \"\"\"Assign some data for just validation\"\"\"\n",
    "        num_validation_points = self.num_training_points \n",
    "        validation_end_index = self.num_training_points + num_validation_points\n",
    "        idx_validation = range(self.num_training_points, validation_end_index -1) # validation indices\n",
    "        model_valid_input = self.y[idx_validation]\n",
    "        model_valid_output = self.x[idx_validation]\n",
    "        return model_valid_input, model_valid_output\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare training and validation data sets\"\"\"\n",
    "        # Training Data\n",
    "        model_training_input, model_training_output = self.training_data()\n",
    "        training_phase = self.phase_vector(model_training_input)\n",
    "\n",
    "        # Validation Data\n",
    "        model_valid_input, model_valid_output = self.validation_data()\n",
    "        valid_phase = self.phase_vector(model_valid_input)\n",
    "        return (model_training_input, model_training_output, training_phase,\n",
    "                model_valid_input, model_valid_output, valid_phase)\n",
    "\n",
    "    def get_model_training_xfc(self):\n",
    "        \"\"\"Build xfc from model training input\"\"\"\n",
    "        return self.build_xfc(self.model_training_input, self.num_memory_levels)\n",
    "    \n",
    "    def get_model_training_expected_output(self):\n",
    "        \"\"\"Find what the model should output for training data\"\"\"\n",
    "        return self.model_expected_output(self.model_training_output, self.training_phase)\n",
    "    \n",
    "    def get_valid_xfc(self):\n",
    "        \"\"\"Build xfc from model validation input\"\"\"\n",
    "        return self.build_xfc(self.model_valid_input, self.num_memory_levels)\n",
    "    \n",
    "    def get_model_valid_expected_output(self):\n",
    "        \"\"\"Find what the model should output for validation data\"\"\"\n",
    "        return self.model_expected_output(self.model_valid_output, self.valid_phase)\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        \"\"\"Get test data after validation\"\"\"\n",
    "        num_validation_points = self.num_training_points \n",
    "        validation_end_index = self.num_training_points + num_validation_points\n",
    "        x_data = self.x[validation_end_index:]\n",
    "        y_data = self.y[validation_end_index:]\n",
    "        return x_data, y_data\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = scipy.io.loadmat(\"PA_IO.mat\")\n",
    "x = data[\"x\"].squeeze()\n",
    "y = data[\"y\"].squeeze()\n",
    "\n",
    "# Create dataset object\n",
    "data_obj = dataset(x, y)\n",
    "\n",
    "# Access training data\n",
    "model_xfc = data_obj.get_model_training_xfc()\n",
    "model_training_expected_output = data_obj.get_model_training_expected_output()\n",
    "\n",
    "# Access validation data\n",
    "valid_xfc = data_obj.get_valid_xfc()\n",
    "model_valid_expected_output = data_obj.get_model_valid_expected_output()\n",
    "\n",
    "# Access test data\n",
    "x_data, y_data = data_obj.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba18ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.63146409-0.11924996j  0.1694142 -0.01547961j -0.29918448+0.23918013j\n",
      "  1.27680917-1.02310172j -0.87359925+0.71303688j  0.12086561+0.23694819j\n",
      "  0.01362539+0.09275574j -0.09982292-0.22003663j  0.45094657+0.5524524j\n",
      " -0.35963882-0.32360673j -0.06705462-0.13809442j -0.02614158-0.01855645j\n",
      "  0.16654034-0.04988737j -0.48981135-0.01449885j  0.34387347+0.01718324j]\n"
     ]
    }
   ],
   "source": [
    "# Get Volterra Model of PA\n",
    "num_memory_levels = 3\n",
    "num_nl_orders = 5\n",
    "A = data_obj.volterra(num_nl_orders)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3184859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/2000  Loss=1.1938e+01  Valid Loss=1.2860e+01\n",
      "Epoch  20/2000  Loss=9.2629e-01  Valid Loss=9.7084e-01\n",
      "Epoch  20/2000  Loss=9.2629e-01  Valid Loss=9.7084e-01\n",
      "Epoch  30/2000  Loss=4.5922e-01  Valid Loss=5.3578e-01\n",
      "Epoch  30/2000  Loss=4.5922e-01  Valid Loss=5.3578e-01\n",
      "Epoch  40/2000  Loss=3.0070e-01  Valid Loss=3.5869e-01\n",
      "Epoch  40/2000  Loss=3.0070e-01  Valid Loss=3.5869e-01\n",
      "Epoch  50/2000  Loss=2.1274e-01  Valid Loss=2.5750e-01\n",
      "Epoch  50/2000  Loss=2.1274e-01  Valid Loss=2.5750e-01\n",
      "Epoch  60/2000  Loss=1.6398e-01  Valid Loss=1.9687e-01\n",
      "Epoch  60/2000  Loss=1.6398e-01  Valid Loss=1.9687e-01\n",
      "Epoch  70/2000  Loss=1.3523e-01  Valid Loss=1.6322e-01\n",
      "Epoch  70/2000  Loss=1.3523e-01  Valid Loss=1.6322e-01\n",
      "Epoch  80/2000  Loss=1.2165e-01  Valid Loss=1.4571e-01\n",
      "Epoch  80/2000  Loss=1.2165e-01  Valid Loss=1.4571e-01\n",
      "Epoch  90/2000  Loss=1.1374e-01  Valid Loss=1.3590e-01\n",
      "Epoch  90/2000  Loss=1.1374e-01  Valid Loss=1.3590e-01\n",
      "Epoch 100/2000  Loss=1.0685e-01  Valid Loss=1.2580e-01\n",
      "Epoch 100/2000  Loss=1.0685e-01  Valid Loss=1.2580e-01\n",
      "Epoch 110/2000  Loss=1.0034e-01  Valid Loss=1.1880e-01\n",
      "Epoch 110/2000  Loss=1.0034e-01  Valid Loss=1.1880e-01\n",
      "Epoch 120/2000  Loss=9.7371e-02  Valid Loss=1.1334e-01\n",
      "Epoch 120/2000  Loss=9.7371e-02  Valid Loss=1.1334e-01\n",
      "Epoch 130/2000  Loss=9.2044e-02  Valid Loss=1.0852e-01\n",
      "Epoch 130/2000  Loss=9.2044e-02  Valid Loss=1.0852e-01\n",
      "Epoch 140/2000  Loss=8.8147e-02  Valid Loss=1.0395e-01\n",
      "Epoch 140/2000  Loss=8.8147e-02  Valid Loss=1.0395e-01\n",
      "Epoch 150/2000  Loss=8.3138e-02  Valid Loss=9.6626e-02\n",
      "Epoch 150/2000  Loss=8.3138e-02  Valid Loss=9.6626e-02\n",
      "Epoch 160/2000  Loss=7.8129e-02  Valid Loss=9.0581e-02\n",
      "Epoch 160/2000  Loss=7.8129e-02  Valid Loss=9.0581e-02\n",
      "Epoch 170/2000  Loss=7.3306e-02  Valid Loss=8.5211e-02\n",
      "Epoch 170/2000  Loss=7.3306e-02  Valid Loss=8.5211e-02\n",
      "Epoch 180/2000  Loss=7.0414e-02  Valid Loss=8.1229e-02\n",
      "Epoch 180/2000  Loss=7.0414e-02  Valid Loss=8.1229e-02\n",
      "Epoch 190/2000  Loss=6.6536e-02  Valid Loss=7.4959e-02\n",
      "Epoch 190/2000  Loss=6.6536e-02  Valid Loss=7.4959e-02\n",
      "Epoch 200/2000  Loss=6.2691e-02  Valid Loss=7.1312e-02\n",
      "Epoch 200/2000  Loss=6.2691e-02  Valid Loss=7.1312e-02\n",
      "Epoch 210/2000  Loss=5.9975e-02  Valid Loss=6.9417e-02\n",
      "Epoch 210/2000  Loss=5.9975e-02  Valid Loss=6.9417e-02\n",
      "Epoch 220/2000  Loss=5.8337e-02  Valid Loss=6.9518e-02\n",
      "Epoch 220/2000  Loss=5.8337e-02  Valid Loss=6.9518e-02\n",
      "Epoch 230/2000  Loss=5.7533e-02  Valid Loss=6.7473e-02\n",
      "Epoch 230/2000  Loss=5.7533e-02  Valid Loss=6.7473e-02\n",
      "Epoch 240/2000  Loss=5.6062e-02  Valid Loss=6.7584e-02\n",
      "Epoch 240/2000  Loss=5.6062e-02  Valid Loss=6.7584e-02\n",
      "Epoch 250/2000  Loss=5.6047e-02  Valid Loss=6.5200e-02\n",
      "Epoch 250/2000  Loss=5.6047e-02  Valid Loss=6.5200e-02\n",
      "Epoch 260/2000  Loss=5.7175e-02  Valid Loss=6.8560e-02\n",
      "Epoch 260/2000  Loss=5.7175e-02  Valid Loss=6.8560e-02\n",
      "Epoch 270/2000  Loss=5.4735e-02  Valid Loss=6.3698e-02\n",
      "Epoch 270/2000  Loss=5.4735e-02  Valid Loss=6.3698e-02\n",
      "Epoch 280/2000  Loss=5.5429e-02  Valid Loss=6.4116e-02\n",
      "Epoch 280/2000  Loss=5.5429e-02  Valid Loss=6.4116e-02\n",
      "Epoch 290/2000  Loss=5.4073e-02  Valid Loss=6.3589e-02\n",
      "Epoch 290/2000  Loss=5.4073e-02  Valid Loss=6.3589e-02\n",
      "Epoch 300/2000  Loss=5.2958e-02  Valid Loss=6.2540e-02\n",
      "Epoch 300/2000  Loss=5.2958e-02  Valid Loss=6.2540e-02\n",
      "Epoch 310/2000  Loss=5.3083e-02  Valid Loss=6.2390e-02\n",
      "Epoch 310/2000  Loss=5.3083e-02  Valid Loss=6.2390e-02\n",
      "Epoch 320/2000  Loss=5.3564e-02  Valid Loss=6.4020e-02\n",
      "Epoch 320/2000  Loss=5.3564e-02  Valid Loss=6.4020e-02\n",
      "Epoch 330/2000  Loss=5.1636e-02  Valid Loss=6.3010e-02\n",
      "Epoch 330/2000  Loss=5.1636e-02  Valid Loss=6.3010e-02\n",
      "Epoch 340/2000  Loss=5.2371e-02  Valid Loss=5.9998e-02\n",
      "Epoch 340/2000  Loss=5.2371e-02  Valid Loss=5.9998e-02\n",
      "Epoch 350/2000  Loss=5.4831e-02  Valid Loss=5.9790e-02\n",
      "Epoch 350/2000  Loss=5.4831e-02  Valid Loss=5.9790e-02\n",
      "Epoch 360/2000  Loss=5.1948e-02  Valid Loss=6.0866e-02\n",
      "Epoch 360/2000  Loss=5.1948e-02  Valid Loss=6.0866e-02\n",
      "Epoch 370/2000  Loss=5.3826e-02  Valid Loss=6.2376e-02\n",
      "Epoch 370/2000  Loss=5.3826e-02  Valid Loss=6.2376e-02\n",
      "Epoch 380/2000  Loss=5.1699e-02  Valid Loss=6.1120e-02\n",
      "Epoch 380/2000  Loss=5.1699e-02  Valid Loss=6.1120e-02\n",
      "Epoch 390/2000  Loss=5.2036e-02  Valid Loss=6.0623e-02\n",
      "Epoch 390/2000  Loss=5.2036e-02  Valid Loss=6.0623e-02\n",
      "Epoch 400/2000  Loss=5.2769e-02  Valid Loss=6.2075e-02\n",
      "Epoch 400/2000  Loss=5.2769e-02  Valid Loss=6.2075e-02\n",
      "Epoch 410/2000  Loss=5.0362e-02  Valid Loss=5.8337e-02\n",
      "Epoch 410/2000  Loss=5.0362e-02  Valid Loss=5.8337e-02\n",
      "Epoch 420/2000  Loss=5.0633e-02  Valid Loss=6.3472e-02\n",
      "Epoch 420/2000  Loss=5.0633e-02  Valid Loss=6.3472e-02\n",
      "Epoch 430/2000  Loss=4.9981e-02  Valid Loss=5.9942e-02\n",
      "Epoch 430/2000  Loss=4.9981e-02  Valid Loss=5.9942e-02\n",
      "Epoch 440/2000  Loss=5.2365e-02  Valid Loss=6.5397e-02\n",
      "Epoch 440/2000  Loss=5.2365e-02  Valid Loss=6.5397e-02\n",
      "Epoch 450/2000  Loss=4.9688e-02  Valid Loss=5.9098e-02\n",
      "Epoch 450/2000  Loss=4.9688e-02  Valid Loss=5.9098e-02\n",
      "Epoch 460/2000  Loss=5.0276e-02  Valid Loss=5.7888e-02\n",
      "Epoch 460/2000  Loss=5.0276e-02  Valid Loss=5.7888e-02\n",
      "Epoch 470/2000  Loss=5.0098e-02  Valid Loss=5.9313e-02\n",
      "Epoch 470/2000  Loss=5.0098e-02  Valid Loss=5.9313e-02\n",
      "Epoch 480/2000  Loss=5.0123e-02  Valid Loss=6.0344e-02\n",
      "Epoch 480/2000  Loss=5.0123e-02  Valid Loss=6.0344e-02\n",
      "Epoch 490/2000  Loss=4.9747e-02  Valid Loss=5.7865e-02\n",
      "Epoch 490/2000  Loss=4.9747e-02  Valid Loss=5.7865e-02\n",
      "Epoch 500/2000  Loss=5.0156e-02  Valid Loss=5.9999e-02\n",
      "Epoch 500/2000  Loss=5.0156e-02  Valid Loss=5.9999e-02\n",
      "Epoch 510/2000  Loss=4.9003e-02  Valid Loss=5.7039e-02\n",
      "Epoch 510/2000  Loss=4.9003e-02  Valid Loss=5.7039e-02\n",
      "Epoch 520/2000  Loss=5.0397e-02  Valid Loss=5.7195e-02\n",
      "Epoch 520/2000  Loss=5.0397e-02  Valid Loss=5.7195e-02\n",
      "Epoch 530/2000  Loss=4.9465e-02  Valid Loss=6.0557e-02\n",
      "Epoch 530/2000  Loss=4.9465e-02  Valid Loss=6.0557e-02\n",
      "Epoch 540/2000  Loss=4.9198e-02  Valid Loss=5.8375e-02\n",
      "Epoch 540/2000  Loss=4.9198e-02  Valid Loss=5.8375e-02\n",
      "Epoch 550/2000  Loss=4.9698e-02  Valid Loss=6.1822e-02\n",
      "Epoch 550/2000  Loss=4.9698e-02  Valid Loss=6.1822e-02\n",
      "Epoch 560/2000  Loss=4.8177e-02  Valid Loss=5.7466e-02\n",
      "Epoch 560/2000  Loss=4.8177e-02  Valid Loss=5.7466e-02\n",
      "Epoch 570/2000  Loss=4.9495e-02  Valid Loss=6.2268e-02\n",
      "Epoch 570/2000  Loss=4.9495e-02  Valid Loss=6.2268e-02\n",
      "Epoch 580/2000  Loss=4.8473e-02  Valid Loss=5.8030e-02\n",
      "Epoch 580/2000  Loss=4.8473e-02  Valid Loss=5.8030e-02\n",
      "Epoch 590/2000  Loss=4.9326e-02  Valid Loss=5.8179e-02\n",
      "Epoch 590/2000  Loss=4.9326e-02  Valid Loss=5.8179e-02\n",
      "Epoch 600/2000  Loss=5.0330e-02  Valid Loss=5.8870e-02\n",
      "Epoch 600/2000  Loss=5.0330e-02  Valid Loss=5.8870e-02\n",
      "Epoch 610/2000  Loss=4.9856e-02  Valid Loss=6.3161e-02\n",
      "Epoch 610/2000  Loss=4.9856e-02  Valid Loss=6.3161e-02\n",
      "Epoch 620/2000  Loss=5.0148e-02  Valid Loss=5.7542e-02\n",
      "Epoch 620/2000  Loss=5.0148e-02  Valid Loss=5.7542e-02\n",
      "Epoch 630/2000  Loss=4.8501e-02  Valid Loss=5.6290e-02\n",
      "Epoch 630/2000  Loss=4.8501e-02  Valid Loss=5.6290e-02\n",
      "Epoch 640/2000  Loss=4.8072e-02  Valid Loss=5.7742e-02\n",
      "Epoch 640/2000  Loss=4.8072e-02  Valid Loss=5.7742e-02\n",
      "Epoch 650/2000  Loss=4.8736e-02  Valid Loss=5.9528e-02\n",
      "Epoch 650/2000  Loss=4.8736e-02  Valid Loss=5.9528e-02\n",
      "Epoch 660/2000  Loss=4.8748e-02  Valid Loss=5.7287e-02\n",
      "Epoch 660/2000  Loss=4.8748e-02  Valid Loss=5.7287e-02\n",
      "Epoch 670/2000  Loss=4.9358e-02  Valid Loss=5.9900e-02\n",
      "Epoch 670/2000  Loss=4.9358e-02  Valid Loss=5.9900e-02\n",
      "Epoch 680/2000  Loss=4.9818e-02  Valid Loss=5.7811e-02\n",
      "Epoch 680/2000  Loss=4.9818e-02  Valid Loss=5.7811e-02\n",
      "Epoch 690/2000  Loss=5.0890e-02  Valid Loss=6.1029e-02\n",
      "Epoch 690/2000  Loss=5.0890e-02  Valid Loss=6.1029e-02\n",
      "Epoch 700/2000  Loss=4.8272e-02  Valid Loss=5.9137e-02\n",
      "Epoch 700/2000  Loss=4.8272e-02  Valid Loss=5.9137e-02\n",
      "Epoch 710/2000  Loss=5.0713e-02  Valid Loss=6.9205e-02\n",
      "Epoch 710/2000  Loss=5.0713e-02  Valid Loss=6.9205e-02\n",
      "Epoch 720/2000  Loss=4.8217e-02  Valid Loss=5.7585e-02\n",
      "Epoch 720/2000  Loss=4.8217e-02  Valid Loss=5.7585e-02\n",
      "Epoch 730/2000  Loss=4.7743e-02  Valid Loss=5.7302e-02\n",
      "Epoch 730/2000  Loss=4.7743e-02  Valid Loss=5.7302e-02\n",
      "Epoch 740/2000  Loss=4.8248e-02  Valid Loss=5.9624e-02\n",
      "Epoch 740/2000  Loss=4.8248e-02  Valid Loss=5.9624e-02\n",
      "Epoch 750/2000  Loss=4.8577e-02  Valid Loss=5.7049e-02\n",
      "Epoch 750/2000  Loss=4.8577e-02  Valid Loss=5.7049e-02\n",
      "Epoch 760/2000  Loss=4.9116e-02  Valid Loss=6.0408e-02\n",
      "Epoch 760/2000  Loss=4.9116e-02  Valid Loss=6.0408e-02\n",
      "Epoch 770/2000  Loss=4.8431e-02  Valid Loss=5.8615e-02\n",
      "Epoch 770/2000  Loss=4.8431e-02  Valid Loss=5.8615e-02\n",
      "Epoch 780/2000  Loss=4.8696e-02  Valid Loss=5.5746e-02\n",
      "Epoch 780/2000  Loss=4.8696e-02  Valid Loss=5.5746e-02\n",
      "Epoch 790/2000  Loss=4.8351e-02  Valid Loss=5.6592e-02\n",
      "Epoch 790/2000  Loss=4.8351e-02  Valid Loss=5.6592e-02\n",
      "Epoch 800/2000  Loss=4.8219e-02  Valid Loss=5.8765e-02\n",
      "Epoch 800/2000  Loss=4.8219e-02  Valid Loss=5.8765e-02\n",
      "Epoch 810/2000  Loss=4.8765e-02  Valid Loss=6.6718e-02\n",
      "Epoch 810/2000  Loss=4.8765e-02  Valid Loss=6.6718e-02\n",
      "Epoch 820/2000  Loss=5.0938e-02  Valid Loss=5.5769e-02\n",
      "Epoch 820/2000  Loss=5.0938e-02  Valid Loss=5.5769e-02\n",
      "Epoch 830/2000  Loss=4.6980e-02  Valid Loss=5.5986e-02\n",
      "Epoch 830/2000  Loss=4.6980e-02  Valid Loss=5.5986e-02\n",
      "Epoch 840/2000  Loss=4.7753e-02  Valid Loss=5.6403e-02\n",
      "Epoch 840/2000  Loss=4.7753e-02  Valid Loss=5.6403e-02\n",
      "Epoch 850/2000  Loss=4.9163e-02  Valid Loss=5.5571e-02\n",
      "Epoch 850/2000  Loss=4.9163e-02  Valid Loss=5.5571e-02\n",
      "Epoch 860/2000  Loss=4.8172e-02  Valid Loss=5.6708e-02\n",
      "Epoch 860/2000  Loss=4.8172e-02  Valid Loss=5.6708e-02\n",
      "Epoch 870/2000  Loss=5.0030e-02  Valid Loss=5.7734e-02\n",
      "Epoch 870/2000  Loss=5.0030e-02  Valid Loss=5.7734e-02\n",
      "Epoch 880/2000  Loss=5.0021e-02  Valid Loss=5.7133e-02\n",
      "Epoch 880/2000  Loss=5.0021e-02  Valid Loss=5.7133e-02\n",
      "Epoch 890/2000  Loss=4.7810e-02  Valid Loss=5.9720e-02\n",
      "Epoch 890/2000  Loss=4.7810e-02  Valid Loss=5.9720e-02\n",
      "Epoch 900/2000  Loss=4.9128e-02  Valid Loss=5.6490e-02\n",
      "Epoch 900/2000  Loss=4.9128e-02  Valid Loss=5.6490e-02\n",
      "Epoch 910/2000  Loss=4.8939e-02  Valid Loss=5.7594e-02\n",
      "Epoch 910/2000  Loss=4.8939e-02  Valid Loss=5.7594e-02\n",
      "Epoch 920/2000  Loss=4.7532e-02  Valid Loss=5.6181e-02\n",
      "Epoch 920/2000  Loss=4.7532e-02  Valid Loss=5.6181e-02\n",
      "Epoch 930/2000  Loss=4.7072e-02  Valid Loss=5.5628e-02\n",
      "Epoch 930/2000  Loss=4.7072e-02  Valid Loss=5.5628e-02\n",
      "Epoch 940/2000  Loss=4.9226e-02  Valid Loss=6.0093e-02\n",
      "Epoch 940/2000  Loss=4.9226e-02  Valid Loss=6.0093e-02\n",
      "Epoch 950/2000  Loss=4.8875e-02  Valid Loss=5.6673e-02\n",
      "Epoch 950/2000  Loss=4.8875e-02  Valid Loss=5.6673e-02\n",
      "Epoch 960/2000  Loss=4.8273e-02  Valid Loss=5.8149e-02\n",
      "Epoch 960/2000  Loss=4.8273e-02  Valid Loss=5.8149e-02\n",
      "Epoch 970/2000  Loss=4.8234e-02  Valid Loss=6.0538e-02\n",
      "Epoch 970/2000  Loss=4.8234e-02  Valid Loss=6.0538e-02\n",
      "Epoch 980/2000  Loss=4.7436e-02  Valid Loss=6.1129e-02\n",
      "Epoch 980/2000  Loss=4.7436e-02  Valid Loss=6.1129e-02\n",
      "Epoch 990/2000  Loss=4.7488e-02  Valid Loss=6.0563e-02\n",
      "Epoch 990/2000  Loss=4.7488e-02  Valid Loss=6.0563e-02\n",
      "Epoch 1000/2000  Loss=4.7040e-02  Valid Loss=5.6445e-02\n",
      "Epoch 1000/2000  Loss=4.7040e-02  Valid Loss=5.6445e-02\n",
      "Epoch 1010/2000  Loss=4.7053e-02  Valid Loss=5.5905e-02\n",
      "Epoch 1010/2000  Loss=4.7053e-02  Valid Loss=5.5905e-02\n",
      "Epoch 1020/2000  Loss=5.0683e-02  Valid Loss=5.7231e-02\n",
      "Epoch 1020/2000  Loss=5.0683e-02  Valid Loss=5.7231e-02\n",
      "Epoch 1030/2000  Loss=4.9839e-02  Valid Loss=5.7057e-02\n",
      "Epoch 1030/2000  Loss=4.9839e-02  Valid Loss=5.7057e-02\n",
      "Epoch 1040/2000  Loss=4.7303e-02  Valid Loss=5.5865e-02\n",
      "Epoch 1040/2000  Loss=4.7303e-02  Valid Loss=5.5865e-02\n",
      "Epoch 1050/2000  Loss=4.6946e-02  Valid Loss=5.9141e-02\n",
      "Epoch 1050/2000  Loss=4.6946e-02  Valid Loss=5.9141e-02\n",
      "Epoch 1060/2000  Loss=4.6968e-02  Valid Loss=6.0632e-02\n",
      "Epoch 1060/2000  Loss=4.6968e-02  Valid Loss=6.0632e-02\n",
      "Epoch 1070/2000  Loss=4.8330e-02  Valid Loss=5.7252e-02\n",
      "Epoch 1070/2000  Loss=4.8330e-02  Valid Loss=5.7252e-02\n",
      "Epoch 1080/2000  Loss=4.7153e-02  Valid Loss=6.1103e-02\n",
      "Epoch 1080/2000  Loss=4.7153e-02  Valid Loss=6.1103e-02\n",
      "Epoch 1090/2000  Loss=4.8005e-02  Valid Loss=5.5175e-02\n",
      "Epoch 1090/2000  Loss=4.8005e-02  Valid Loss=5.5175e-02\n",
      "Epoch 1100/2000  Loss=4.7817e-02  Valid Loss=5.7594e-02\n",
      "Epoch 1100/2000  Loss=4.7817e-02  Valid Loss=5.7594e-02\n",
      "Epoch 1110/2000  Loss=4.8341e-02  Valid Loss=5.6906e-02\n",
      "Epoch 1110/2000  Loss=4.8341e-02  Valid Loss=5.6906e-02\n",
      "Epoch 1120/2000  Loss=4.5950e-02  Valid Loss=5.6086e-02\n",
      "Epoch 1120/2000  Loss=4.5950e-02  Valid Loss=5.6086e-02\n",
      "Epoch 1130/2000  Loss=4.6292e-02  Valid Loss=5.6843e-02\n",
      "Epoch 1130/2000  Loss=4.6292e-02  Valid Loss=5.6843e-02\n",
      "Epoch 1140/2000  Loss=4.7656e-02  Valid Loss=5.5734e-02\n",
      "Epoch 1140/2000  Loss=4.7656e-02  Valid Loss=5.5734e-02\n",
      "Epoch 1150/2000  Loss=4.6982e-02  Valid Loss=5.7092e-02\n",
      "Epoch 1150/2000  Loss=4.6982e-02  Valid Loss=5.7092e-02\n",
      "Epoch 1160/2000  Loss=4.9081e-02  Valid Loss=5.4992e-02\n",
      "Epoch 1160/2000  Loss=4.9081e-02  Valid Loss=5.4992e-02\n",
      "Epoch 1170/2000  Loss=4.6589e-02  Valid Loss=5.6055e-02\n",
      "Epoch 1170/2000  Loss=4.6589e-02  Valid Loss=5.6055e-02\n",
      "Epoch 1180/2000  Loss=5.0717e-02  Valid Loss=5.9741e-02\n",
      "Epoch 1180/2000  Loss=5.0717e-02  Valid Loss=5.9741e-02\n",
      "Epoch 1190/2000  Loss=4.7075e-02  Valid Loss=5.4618e-02\n",
      "Epoch 1190/2000  Loss=4.7075e-02  Valid Loss=5.4618e-02\n",
      "Epoch 1200/2000  Loss=4.7147e-02  Valid Loss=6.3273e-02\n",
      "Epoch 1200/2000  Loss=4.7147e-02  Valid Loss=6.3273e-02\n",
      "Epoch 1210/2000  Loss=4.9211e-02  Valid Loss=6.8505e-02\n",
      "Epoch 1210/2000  Loss=4.9211e-02  Valid Loss=6.8505e-02\n",
      "Epoch 1220/2000  Loss=4.7541e-02  Valid Loss=5.7088e-02\n",
      "Epoch 1220/2000  Loss=4.7541e-02  Valid Loss=5.7088e-02\n",
      "Epoch 1230/2000  Loss=4.8640e-02  Valid Loss=5.4682e-02\n",
      "Epoch 1230/2000  Loss=4.8640e-02  Valid Loss=5.4682e-02\n",
      "Epoch 1240/2000  Loss=4.8487e-02  Valid Loss=6.5601e-02\n",
      "Epoch 1240/2000  Loss=4.8487e-02  Valid Loss=6.5601e-02\n",
      "Epoch 1250/2000  Loss=4.5966e-02  Valid Loss=5.8213e-02\n",
      "Epoch 1250/2000  Loss=4.5966e-02  Valid Loss=5.8213e-02\n",
      "Epoch 1260/2000  Loss=4.8895e-02  Valid Loss=5.9575e-02\n",
      "Epoch 1260/2000  Loss=4.8895e-02  Valid Loss=5.9575e-02\n",
      "Epoch 1270/2000  Loss=4.8751e-02  Valid Loss=5.8790e-02\n",
      "Epoch 1270/2000  Loss=4.8751e-02  Valid Loss=5.8790e-02\n",
      "Epoch 1280/2000  Loss=4.7104e-02  Valid Loss=6.3223e-02\n",
      "Epoch 1280/2000  Loss=4.7104e-02  Valid Loss=6.3223e-02\n",
      "Epoch 1290/2000  Loss=4.6388e-02  Valid Loss=5.7662e-02\n",
      "Epoch 1290/2000  Loss=4.6388e-02  Valid Loss=5.7662e-02\n",
      "Epoch 1300/2000  Loss=4.7725e-02  Valid Loss=6.3706e-02\n",
      "Epoch 1300/2000  Loss=4.7725e-02  Valid Loss=6.3706e-02\n",
      "Epoch 1310/2000  Loss=4.6598e-02  Valid Loss=5.5629e-02\n",
      "Epoch 1310/2000  Loss=4.6598e-02  Valid Loss=5.5629e-02\n",
      "Epoch 1320/2000  Loss=4.6503e-02  Valid Loss=5.5340e-02\n",
      "Epoch 1320/2000  Loss=4.6503e-02  Valid Loss=5.5340e-02\n",
      "Epoch 1330/2000  Loss=4.5936e-02  Valid Loss=5.7214e-02\n",
      "Epoch 1330/2000  Loss=4.5936e-02  Valid Loss=5.7214e-02\n",
      "Epoch 1340/2000  Loss=4.8605e-02  Valid Loss=5.8160e-02\n",
      "Epoch 1340/2000  Loss=4.8605e-02  Valid Loss=5.8160e-02\n",
      "Epoch 1350/2000  Loss=4.6386e-02  Valid Loss=5.5491e-02\n",
      "Epoch 1350/2000  Loss=4.6386e-02  Valid Loss=5.5491e-02\n",
      "Epoch 1360/2000  Loss=4.5928e-02  Valid Loss=5.6875e-02\n",
      "Epoch 1360/2000  Loss=4.5928e-02  Valid Loss=5.6875e-02\n",
      "Epoch 1370/2000  Loss=4.9199e-02  Valid Loss=5.7832e-02\n",
      "Epoch 1370/2000  Loss=4.9199e-02  Valid Loss=5.7832e-02\n",
      "Epoch 1380/2000  Loss=4.8858e-02  Valid Loss=5.6812e-02\n",
      "Epoch 1380/2000  Loss=4.8858e-02  Valid Loss=5.6812e-02\n",
      "Epoch 1390/2000  Loss=4.6582e-02  Valid Loss=5.5380e-02\n",
      "Epoch 1390/2000  Loss=4.6582e-02  Valid Loss=5.5380e-02\n",
      "Epoch 1400/2000  Loss=4.5849e-02  Valid Loss=6.0549e-02\n",
      "Epoch 1400/2000  Loss=4.5849e-02  Valid Loss=6.0549e-02\n",
      "Epoch 1410/2000  Loss=4.6544e-02  Valid Loss=5.6977e-02\n",
      "Epoch 1410/2000  Loss=4.6544e-02  Valid Loss=5.6977e-02\n",
      "Epoch 1420/2000  Loss=4.9057e-02  Valid Loss=5.9400e-02\n",
      "Epoch 1420/2000  Loss=4.9057e-02  Valid Loss=5.9400e-02\n",
      "Epoch 1430/2000  Loss=4.6621e-02  Valid Loss=6.1299e-02\n",
      "Epoch 1430/2000  Loss=4.6621e-02  Valid Loss=6.1299e-02\n",
      "Epoch 1440/2000  Loss=4.6824e-02  Valid Loss=5.4674e-02\n",
      "Epoch 1440/2000  Loss=4.6824e-02  Valid Loss=5.4674e-02\n",
      "Epoch 1450/2000  Loss=4.5647e-02  Valid Loss=6.0514e-02\n",
      "Epoch 1450/2000  Loss=4.5647e-02  Valid Loss=6.0514e-02\n",
      "Epoch 1460/2000  Loss=4.6622e-02  Valid Loss=5.7426e-02\n",
      "Epoch 1460/2000  Loss=4.6622e-02  Valid Loss=5.7426e-02\n",
      "Epoch 1470/2000  Loss=4.6298e-02  Valid Loss=5.5802e-02\n",
      "Epoch 1470/2000  Loss=4.6298e-02  Valid Loss=5.5802e-02\n",
      "Epoch 1480/2000  Loss=4.6675e-02  Valid Loss=5.4687e-02\n",
      "Epoch 1480/2000  Loss=4.6675e-02  Valid Loss=5.4687e-02\n",
      "Epoch 1490/2000  Loss=4.6230e-02  Valid Loss=5.5707e-02\n",
      "Epoch 1490/2000  Loss=4.6230e-02  Valid Loss=5.5707e-02\n",
      "Epoch 1500/2000  Loss=4.5896e-02  Valid Loss=5.4587e-02\n",
      "Epoch 1500/2000  Loss=4.5896e-02  Valid Loss=5.4587e-02\n",
      "Epoch 1510/2000  Loss=4.7863e-02  Valid Loss=5.5462e-02\n",
      "Epoch 1510/2000  Loss=4.7863e-02  Valid Loss=5.5462e-02\n",
      "Epoch 1520/2000  Loss=4.6588e-02  Valid Loss=5.6853e-02\n",
      "Epoch 1520/2000  Loss=4.6588e-02  Valid Loss=5.6853e-02\n",
      "Epoch 1530/2000  Loss=4.6620e-02  Valid Loss=6.0323e-02\n",
      "Epoch 1530/2000  Loss=4.6620e-02  Valid Loss=6.0323e-02\n",
      "Epoch 1540/2000  Loss=4.7627e-02  Valid Loss=5.6550e-02\n",
      "Epoch 1540/2000  Loss=4.7627e-02  Valid Loss=5.6550e-02\n",
      "Epoch 1550/2000  Loss=4.6946e-02  Valid Loss=6.0764e-02\n",
      "Epoch 1550/2000  Loss=4.6946e-02  Valid Loss=6.0764e-02\n",
      "Epoch 1560/2000  Loss=4.5420e-02  Valid Loss=5.5192e-02\n",
      "Epoch 1560/2000  Loss=4.5420e-02  Valid Loss=5.5192e-02\n",
      "Epoch 1570/2000  Loss=4.5418e-02  Valid Loss=5.5263e-02\n",
      "Epoch 1570/2000  Loss=4.5418e-02  Valid Loss=5.5263e-02\n",
      "Epoch 1580/2000  Loss=4.7243e-02  Valid Loss=5.5322e-02\n",
      "Epoch 1580/2000  Loss=4.7243e-02  Valid Loss=5.5322e-02\n",
      "Epoch 1590/2000  Loss=4.5679e-02  Valid Loss=5.3879e-02\n",
      "Epoch 1590/2000  Loss=4.5679e-02  Valid Loss=5.3879e-02\n",
      "Epoch 1600/2000  Loss=4.5780e-02  Valid Loss=5.9868e-02\n",
      "Epoch 1600/2000  Loss=4.5780e-02  Valid Loss=5.9868e-02\n",
      "Epoch 1610/2000  Loss=4.6072e-02  Valid Loss=5.5512e-02\n",
      "Epoch 1610/2000  Loss=4.6072e-02  Valid Loss=5.5512e-02\n",
      "Epoch 1620/2000  Loss=4.7241e-02  Valid Loss=5.4585e-02\n",
      "Epoch 1620/2000  Loss=4.7241e-02  Valid Loss=5.4585e-02\n",
      "Epoch 1630/2000  Loss=5.0269e-02  Valid Loss=5.8522e-02\n",
      "Epoch 1630/2000  Loss=5.0269e-02  Valid Loss=5.8522e-02\n",
      "Epoch 1640/2000  Loss=4.6149e-02  Valid Loss=5.4614e-02\n",
      "Epoch 1640/2000  Loss=4.6149e-02  Valid Loss=5.4614e-02\n",
      "Epoch 1650/2000  Loss=4.5897e-02  Valid Loss=5.4777e-02\n",
      "Epoch 1650/2000  Loss=4.5897e-02  Valid Loss=5.4777e-02\n",
      "Epoch 1660/2000  Loss=4.5857e-02  Valid Loss=5.7695e-02\n",
      "Epoch 1660/2000  Loss=4.5857e-02  Valid Loss=5.7695e-02\n",
      "Epoch 1670/2000  Loss=4.7328e-02  Valid Loss=5.4027e-02\n",
      "Epoch 1670/2000  Loss=4.7328e-02  Valid Loss=5.4027e-02\n",
      "Epoch 1680/2000  Loss=4.8107e-02  Valid Loss=6.0999e-02\n",
      "Epoch 1680/2000  Loss=4.8107e-02  Valid Loss=6.0999e-02\n",
      "Epoch 1690/2000  Loss=4.6796e-02  Valid Loss=5.4487e-02\n",
      "Epoch 1690/2000  Loss=4.6796e-02  Valid Loss=5.4487e-02\n",
      "Epoch 1700/2000  Loss=4.7439e-02  Valid Loss=5.3891e-02\n",
      "Epoch 1700/2000  Loss=4.7439e-02  Valid Loss=5.3891e-02\n",
      "Epoch 1710/2000  Loss=4.7346e-02  Valid Loss=5.5150e-02\n",
      "Epoch 1710/2000  Loss=4.7346e-02  Valid Loss=5.5150e-02\n",
      "Epoch 1720/2000  Loss=4.6962e-02  Valid Loss=6.1482e-02\n",
      "Epoch 1720/2000  Loss=4.6962e-02  Valid Loss=6.1482e-02\n",
      "Epoch 1730/2000  Loss=4.5873e-02  Valid Loss=5.8017e-02\n",
      "Epoch 1730/2000  Loss=4.5873e-02  Valid Loss=5.8017e-02\n",
      "Epoch 1740/2000  Loss=4.7178e-02  Valid Loss=5.4733e-02\n",
      "Epoch 1740/2000  Loss=4.7178e-02  Valid Loss=5.4733e-02\n",
      "Epoch 1750/2000  Loss=4.6249e-02  Valid Loss=5.8798e-02\n",
      "Epoch 1750/2000  Loss=4.6249e-02  Valid Loss=5.8798e-02\n",
      "Epoch 1760/2000  Loss=4.7132e-02  Valid Loss=5.5647e-02\n",
      "Epoch 1760/2000  Loss=4.7132e-02  Valid Loss=5.5647e-02\n",
      "Epoch 1770/2000  Loss=4.6042e-02  Valid Loss=5.7453e-02\n",
      "Epoch 1770/2000  Loss=4.6042e-02  Valid Loss=5.7453e-02\n",
      "Epoch 1780/2000  Loss=4.5608e-02  Valid Loss=5.3918e-02\n",
      "Epoch 1780/2000  Loss=4.5608e-02  Valid Loss=5.3918e-02\n",
      "Epoch 1790/2000  Loss=4.6221e-02  Valid Loss=5.5219e-02\n",
      "Epoch 1790/2000  Loss=4.6221e-02  Valid Loss=5.5219e-02\n",
      "Epoch 1800/2000  Loss=4.5433e-02  Valid Loss=5.5058e-02\n",
      "Epoch 1800/2000  Loss=4.5433e-02  Valid Loss=5.5058e-02\n",
      "Epoch 1810/2000  Loss=4.5697e-02  Valid Loss=5.8231e-02\n",
      "Epoch 1810/2000  Loss=4.5697e-02  Valid Loss=5.8231e-02\n",
      "Epoch 1820/2000  Loss=4.6719e-02  Valid Loss=5.6406e-02\n",
      "Epoch 1820/2000  Loss=4.6719e-02  Valid Loss=5.6406e-02\n",
      "Epoch 1830/2000  Loss=4.5751e-02  Valid Loss=5.9011e-02\n",
      "Epoch 1830/2000  Loss=4.5751e-02  Valid Loss=5.9011e-02\n",
      "Epoch 1840/2000  Loss=4.6304e-02  Valid Loss=5.6066e-02\n",
      "Epoch 1840/2000  Loss=4.6304e-02  Valid Loss=5.6066e-02\n",
      "Epoch 1850/2000  Loss=4.7681e-02  Valid Loss=5.5366e-02\n",
      "Epoch 1850/2000  Loss=4.7681e-02  Valid Loss=5.5366e-02\n",
      "Epoch 1860/2000  Loss=4.6305e-02  Valid Loss=5.6226e-02\n",
      "Epoch 1860/2000  Loss=4.6305e-02  Valid Loss=5.6226e-02\n",
      "Epoch 1870/2000  Loss=4.6064e-02  Valid Loss=5.6362e-02\n",
      "Epoch 1870/2000  Loss=4.6064e-02  Valid Loss=5.6362e-02\n",
      "Epoch 1880/2000  Loss=4.6037e-02  Valid Loss=5.7204e-02\n",
      "Epoch 1880/2000  Loss=4.6037e-02  Valid Loss=5.7204e-02\n",
      "Epoch 1890/2000  Loss=4.6184e-02  Valid Loss=6.1800e-02\n",
      "Epoch 1890/2000  Loss=4.6184e-02  Valid Loss=6.1800e-02\n",
      "Epoch 1900/2000  Loss=4.8734e-02  Valid Loss=5.6350e-02\n",
      "Epoch 1900/2000  Loss=4.8734e-02  Valid Loss=5.6350e-02\n",
      "Epoch 1910/2000  Loss=4.6135e-02  Valid Loss=5.3859e-02\n",
      "Epoch 1910/2000  Loss=4.6135e-02  Valid Loss=5.3859e-02\n",
      "Epoch 1920/2000  Loss=4.5112e-02  Valid Loss=5.7614e-02\n",
      "Epoch 1920/2000  Loss=4.5112e-02  Valid Loss=5.7614e-02\n",
      "Epoch 1930/2000  Loss=5.0112e-02  Valid Loss=6.1499e-02\n",
      "Epoch 1930/2000  Loss=5.0112e-02  Valid Loss=6.1499e-02\n",
      "Epoch 1940/2000  Loss=4.7074e-02  Valid Loss=5.4783e-02\n",
      "Epoch 1940/2000  Loss=4.7074e-02  Valid Loss=5.4783e-02\n",
      "Epoch 1950/2000  Loss=4.5579e-02  Valid Loss=5.5698e-02\n",
      "Epoch 1950/2000  Loss=4.5579e-02  Valid Loss=5.5698e-02\n",
      "Epoch 1960/2000  Loss=4.5267e-02  Valid Loss=5.3465e-02\n",
      "Epoch 1960/2000  Loss=4.5267e-02  Valid Loss=5.3465e-02\n",
      "Epoch 1970/2000  Loss=4.6547e-02  Valid Loss=5.5278e-02\n",
      "Epoch 1970/2000  Loss=4.6547e-02  Valid Loss=5.5278e-02\n",
      "Epoch 1980/2000  Loss=4.5584e-02  Valid Loss=5.5620e-02\n",
      "Epoch 1980/2000  Loss=4.5584e-02  Valid Loss=5.5620e-02\n",
      "Epoch 1990/2000  Loss=4.5632e-02  Valid Loss=5.6583e-02\n",
      "Epoch 1990/2000  Loss=4.5632e-02  Valid Loss=5.6583e-02\n",
      "Epoch 2000/2000  Loss=4.6433e-02  Valid Loss=5.5689e-02\n",
      "\n",
      "Best model from epoch 1984 with validation loss: 5.3419e-02\n",
      "-37.524947315279\n",
      "Epoch 2000/2000  Loss=4.6433e-02  Valid Loss=5.5689e-02\n",
      "\n",
      "Best model from epoch 1984 with validation loss: 5.3419e-02\n",
      "-37.524947315279\n"
     ]
    }
   ],
   "source": [
    "# Train NN on backprop PA for inv model\n",
    "\n",
    "class PNTDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PNTDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, pntdnn):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pntdnn = pntdnn\n",
    "        \n",
    "    def build_dataloaders(self, x , y):\n",
    "        X = torch.tensor(x, dtype=torch.float32)\n",
    "        Y = torch.tensor(y, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X, Y)\n",
    "        loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "        return loader\n",
    "\n",
    "    \n",
    "    def get_best_model(self, train_loader, valid_loader, num_epochs=400, learning_rate=1e-3):\n",
    "        \"\"\"Train model and return the best model based on validation loss\"\"\"\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.pntdnn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        best_valid_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.pntdnn.train()\n",
    "            running_train_loss = 0\n",
    "            running_valid_loss = 0\n",
    "            \n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds = self.pntdnn(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_train_loss += loss.item() * xb.size(0)\n",
    "                \n",
    "            train_loss = running_train_loss\n",
    "            \n",
    "            self.pntdnn.eval()\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in valid_loader:\n",
    "                    preds = self.pntdnn(xb)\n",
    "                    loss = criterion(preds, yb)\n",
    "                    running_valid_loss += loss.item() * xb.size(0)\n",
    "                \n",
    "            valid_loss = running_valid_loss\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                best_model_state = copy.deepcopy(self.pntdnn.state_dict())\n",
    "                best_epoch = epoch + 1\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1:3d}/{num_epochs}  Loss={train_loss:.4e}  Valid Loss={valid_loss:.4e}\")\n",
    "        \n",
    "        # Load best model\n",
    "        self.pntdnn.load_state_dict(best_model_state)\n",
    "        print(f\"\\nBest model from epoch {best_epoch} with validation loss: {best_valid_loss:.4e}\")\n",
    "        \n",
    "        return train_losses, valid_losses, best_epoch\n",
    "\n",
    "    def prune_model(self, prune_amount=0.2):\n",
    "        pruned_model = copy.deepcopy(self.pntdnn)\n",
    "        parameters_to_prune = (\n",
    "            (pruned_model.fc1, 'weight'),\n",
    "            (pruned_model.fc2, 'weight'),\n",
    "        )\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=prune_amount,\n",
    "        )\n",
    "        return pruned_model\n",
    "    \n",
    "    def calculate_nmse(self, x, y):\n",
    "        \"\"\"Return NMSE in dB\"\"\"\n",
    "        self.pntdnn.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(x, dtype=torch.float32)\n",
    "            targets = torch.tensor(y, dtype=torch.float32)\n",
    "            outputs = self.pntdnn(inputs)\n",
    "            mse_loss = nn.MSELoss()(outputs, targets).item()\n",
    "            signal_power = torch.mean(targets ** 2).item()\n",
    "            nmse = mse_loss / signal_power\n",
    "            nmse = 10 * np.log10(nmse)\n",
    "        return nmse\n",
    "    \n",
    "\n",
    "# Instantiate and train the model\n",
    "input_size = model_xfc.shape[1]\n",
    "hidden_size = 12\n",
    "output_size = 2\n",
    "pntdnn = PNTDNN(input_size, hidden_size, output_size)\n",
    "nn_model = NN(pntdnn)\n",
    "train_loader = nn_model.build_dataloaders(model_xfc, model_training_expected_output)\n",
    "valid_loader = nn_model.build_dataloaders(valid_xfc, model_valid_expected_output)\n",
    "train_losses, valid_losses, best_epoch = nn_model.get_best_model(train_loader, valid_loader, num_epochs=2000)\n",
    "print(nn_model.calculate_nmse(model_xfc, model_training_expected_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/200  Loss=4.8473e-02  Valid Loss=5.7157e-02\n",
      "Epoch  20/200  Loss=4.6755e-02  Valid Loss=5.4827e-02\n",
      "Epoch  20/200  Loss=4.6755e-02  Valid Loss=5.4827e-02\n",
      "Epoch  30/200  Loss=4.6196e-02  Valid Loss=5.3945e-02\n",
      "Epoch  30/200  Loss=4.6196e-02  Valid Loss=5.3945e-02\n",
      "Epoch  40/200  Loss=4.5411e-02  Valid Loss=5.6006e-02\n",
      "Epoch  40/200  Loss=4.5411e-02  Valid Loss=5.6006e-02\n",
      "Epoch  50/200  Loss=4.8200e-02  Valid Loss=5.4007e-02\n",
      "Epoch  50/200  Loss=4.8200e-02  Valid Loss=5.4007e-02\n",
      "Epoch  60/200  Loss=4.6152e-02  Valid Loss=5.6797e-02\n",
      "Epoch  60/200  Loss=4.6152e-02  Valid Loss=5.6797e-02\n",
      "Epoch  70/200  Loss=4.6317e-02  Valid Loss=5.6623e-02\n",
      "Epoch  70/200  Loss=4.6317e-02  Valid Loss=5.6623e-02\n",
      "Epoch  80/200  Loss=4.4862e-02  Valid Loss=5.6525e-02\n",
      "Epoch  80/200  Loss=4.4862e-02  Valid Loss=5.6525e-02\n",
      "Epoch  90/200  Loss=4.5702e-02  Valid Loss=5.9120e-02\n",
      "Epoch  90/200  Loss=4.5702e-02  Valid Loss=5.9120e-02\n",
      "Epoch 100/200  Loss=5.0929e-02  Valid Loss=5.6336e-02\n",
      "Epoch 100/200  Loss=5.0929e-02  Valid Loss=5.6336e-02\n",
      "Epoch 110/200  Loss=4.4949e-02  Valid Loss=5.4450e-02\n",
      "Epoch 110/200  Loss=4.4949e-02  Valid Loss=5.4450e-02\n",
      "Epoch 120/200  Loss=4.6902e-02  Valid Loss=5.4790e-02\n",
      "Epoch 120/200  Loss=4.6902e-02  Valid Loss=5.4790e-02\n",
      "Epoch 130/200  Loss=4.5669e-02  Valid Loss=5.7346e-02\n",
      "Epoch 130/200  Loss=4.5669e-02  Valid Loss=5.7346e-02\n",
      "Epoch 140/200  Loss=4.6903e-02  Valid Loss=5.4802e-02\n",
      "Epoch 140/200  Loss=4.6903e-02  Valid Loss=5.4802e-02\n",
      "Epoch 150/200  Loss=4.5384e-02  Valid Loss=5.3751e-02\n",
      "Epoch 150/200  Loss=4.5384e-02  Valid Loss=5.3751e-02\n",
      "Epoch 160/200  Loss=4.5923e-02  Valid Loss=5.4129e-02\n",
      "Epoch 160/200  Loss=4.5923e-02  Valid Loss=5.4129e-02\n",
      "Epoch 170/200  Loss=4.6434e-02  Valid Loss=5.8347e-02\n",
      "Epoch 170/200  Loss=4.6434e-02  Valid Loss=5.8347e-02\n",
      "Epoch 180/200  Loss=4.7647e-02  Valid Loss=6.0611e-02\n",
      "Epoch 180/200  Loss=4.7647e-02  Valid Loss=6.0611e-02\n",
      "Epoch 190/200  Loss=4.5196e-02  Valid Loss=5.5551e-02\n",
      "Epoch 190/200  Loss=4.5196e-02  Valid Loss=5.5551e-02\n",
      "Epoch 200/200  Loss=4.5855e-02  Valid Loss=5.5316e-02\n",
      "\n",
      "Best model from epoch 139 with validation loss: 5.3378e-02\n",
      "Pruned Model NMSE at sparsity 0.2: -37.518105635736305 dB\n",
      "Epoch 200/200  Loss=4.5855e-02  Valid Loss=5.5316e-02\n",
      "\n",
      "Best model from epoch 139 with validation loss: 5.3378e-02\n",
      "Pruned Model NMSE at sparsity 0.2: -37.518105635736305 dB\n",
      "Epoch  10/200  Loss=1.5441e-01  Valid Loss=1.7656e-01\n",
      "Epoch  10/200  Loss=1.5441e-01  Valid Loss=1.7656e-01\n",
      "Epoch  20/200  Loss=1.0888e-01  Valid Loss=1.3194e-01\n",
      "Epoch  20/200  Loss=1.0888e-01  Valid Loss=1.3194e-01\n",
      "Epoch  30/200  Loss=8.5455e-02  Valid Loss=1.0752e-01\n",
      "Epoch  30/200  Loss=8.5455e-02  Valid Loss=1.0752e-01\n",
      "Epoch  40/200  Loss=7.2940e-02  Valid Loss=9.0415e-02\n",
      "Epoch  40/200  Loss=7.2940e-02  Valid Loss=9.0415e-02\n",
      "Epoch  50/200  Loss=6.7199e-02  Valid Loss=8.1695e-02\n",
      "Epoch  50/200  Loss=6.7199e-02  Valid Loss=8.1695e-02\n",
      "Epoch  60/200  Loss=6.4813e-02  Valid Loss=8.1782e-02\n",
      "Epoch  60/200  Loss=6.4813e-02  Valid Loss=8.1782e-02\n",
      "Epoch  70/200  Loss=6.2888e-02  Valid Loss=7.7966e-02\n",
      "Epoch  70/200  Loss=6.2888e-02  Valid Loss=7.7966e-02\n",
      "Epoch  80/200  Loss=6.1360e-02  Valid Loss=7.3362e-02\n",
      "Epoch  80/200  Loss=6.1360e-02  Valid Loss=7.3362e-02\n",
      "Epoch  90/200  Loss=6.0924e-02  Valid Loss=7.2302e-02\n",
      "Epoch  90/200  Loss=6.0924e-02  Valid Loss=7.2302e-02\n",
      "Epoch 100/200  Loss=6.1356e-02  Valid Loss=7.2751e-02\n",
      "Epoch 100/200  Loss=6.1356e-02  Valid Loss=7.2751e-02\n",
      "Epoch 110/200  Loss=6.1202e-02  Valid Loss=7.1343e-02\n",
      "Epoch 110/200  Loss=6.1202e-02  Valid Loss=7.1343e-02\n",
      "Epoch 120/200  Loss=6.0726e-02  Valid Loss=7.0583e-02\n",
      "Epoch 120/200  Loss=6.0726e-02  Valid Loss=7.0583e-02\n",
      "Epoch 130/200  Loss=6.0816e-02  Valid Loss=7.0255e-02\n",
      "Epoch 130/200  Loss=6.0816e-02  Valid Loss=7.0255e-02\n",
      "Epoch 140/200  Loss=5.9698e-02  Valid Loss=7.1176e-02\n",
      "Epoch 140/200  Loss=5.9698e-02  Valid Loss=7.1176e-02\n",
      "Epoch 150/200  Loss=5.9810e-02  Valid Loss=7.1112e-02\n",
      "Epoch 150/200  Loss=5.9810e-02  Valid Loss=7.1112e-02\n",
      "Epoch 160/200  Loss=6.0692e-02  Valid Loss=7.0864e-02\n",
      "Epoch 160/200  Loss=6.0692e-02  Valid Loss=7.0864e-02\n",
      "Epoch 170/200  Loss=6.1663e-02  Valid Loss=7.0532e-02\n",
      "Epoch 170/200  Loss=6.1663e-02  Valid Loss=7.0532e-02\n",
      "Epoch 180/200  Loss=6.0007e-02  Valid Loss=7.1249e-02\n",
      "Epoch 180/200  Loss=6.0007e-02  Valid Loss=7.1249e-02\n",
      "Epoch 190/200  Loss=6.0124e-02  Valid Loss=6.9671e-02\n",
      "Epoch 190/200  Loss=6.0124e-02  Valid Loss=6.9671e-02\n",
      "Epoch 200/200  Loss=6.0870e-02  Valid Loss=7.2167e-02\n",
      "\n",
      "Best model from epoch 190 with validation loss: 6.9671e-02\n",
      "Pruned Model NMSE at sparsity 0.4: -36.30390066457481 dB\n",
      "Epoch 200/200  Loss=6.0870e-02  Valid Loss=7.2167e-02\n",
      "\n",
      "Best model from epoch 190 with validation loss: 6.9671e-02\n",
      "Pruned Model NMSE at sparsity 0.4: -36.30390066457481 dB\n",
      "Epoch  10/200  Loss=3.5557e-01  Valid Loss=3.9380e-01\n",
      "Epoch  10/200  Loss=3.5557e-01  Valid Loss=3.9380e-01\n",
      "Epoch  20/200  Loss=1.5665e-01  Valid Loss=1.8327e-01\n",
      "Epoch  20/200  Loss=1.5665e-01  Valid Loss=1.8327e-01\n",
      "Epoch  30/200  Loss=1.0946e-01  Valid Loss=1.3035e-01\n",
      "Epoch  30/200  Loss=1.0946e-01  Valid Loss=1.3035e-01\n",
      "Epoch  40/200  Loss=9.5640e-02  Valid Loss=1.1383e-01\n",
      "Epoch  40/200  Loss=9.5640e-02  Valid Loss=1.1383e-01\n",
      "Epoch  50/200  Loss=8.8140e-02  Valid Loss=1.0507e-01\n",
      "Epoch  50/200  Loss=8.8140e-02  Valid Loss=1.0507e-01\n",
      "Epoch  60/200  Loss=8.5061e-02  Valid Loss=9.8548e-02\n",
      "Epoch  60/200  Loss=8.5061e-02  Valid Loss=9.8548e-02\n",
      "Epoch  70/200  Loss=8.1707e-02  Valid Loss=9.5408e-02\n",
      "Epoch  70/200  Loss=8.1707e-02  Valid Loss=9.5408e-02\n",
      "Epoch  80/200  Loss=7.8810e-02  Valid Loss=9.3465e-02\n",
      "Epoch  80/200  Loss=7.8810e-02  Valid Loss=9.3465e-02\n",
      "Epoch  90/200  Loss=7.8790e-02  Valid Loss=9.2790e-02\n",
      "Epoch  90/200  Loss=7.8790e-02  Valid Loss=9.2790e-02\n",
      "Epoch 100/200  Loss=7.8369e-02  Valid Loss=9.1080e-02\n",
      "Epoch 100/200  Loss=7.8369e-02  Valid Loss=9.1080e-02\n",
      "Epoch 110/200  Loss=7.6763e-02  Valid Loss=9.0398e-02\n",
      "Epoch 110/200  Loss=7.6763e-02  Valid Loss=9.0398e-02\n",
      "Epoch 120/200  Loss=7.6570e-02  Valid Loss=9.0778e-02\n",
      "Epoch 120/200  Loss=7.6570e-02  Valid Loss=9.0778e-02\n",
      "Epoch 130/200  Loss=7.7382e-02  Valid Loss=9.0609e-02\n",
      "Epoch 130/200  Loss=7.7382e-02  Valid Loss=9.0609e-02\n",
      "Epoch 140/200  Loss=7.6524e-02  Valid Loss=9.0494e-02\n",
      "Epoch 140/200  Loss=7.6524e-02  Valid Loss=9.0494e-02\n",
      "Epoch 150/200  Loss=7.7033e-02  Valid Loss=8.9447e-02\n",
      "Epoch 150/200  Loss=7.7033e-02  Valid Loss=8.9447e-02\n",
      "Epoch 160/200  Loss=7.7905e-02  Valid Loss=8.9441e-02\n",
      "Epoch 160/200  Loss=7.7905e-02  Valid Loss=8.9441e-02\n",
      "Epoch 170/200  Loss=7.6516e-02  Valid Loss=9.0012e-02\n",
      "Epoch 170/200  Loss=7.6516e-02  Valid Loss=9.0012e-02\n",
      "Epoch 180/200  Loss=7.6089e-02  Valid Loss=8.9103e-02\n",
      "Epoch 180/200  Loss=7.6089e-02  Valid Loss=8.9103e-02\n",
      "Epoch 190/200  Loss=7.8187e-02  Valid Loss=9.1790e-02\n",
      "Epoch 190/200  Loss=7.8187e-02  Valid Loss=9.1790e-02\n",
      "Epoch 200/200  Loss=7.7460e-02  Valid Loss=9.3643e-02\n",
      "\n",
      "Best model from epoch 199 with validation loss: 8.7599e-02\n",
      "Pruned Model NMSE at sparsity 0.6: -35.2588298605587 dB\n",
      "Epoch 200/200  Loss=7.7460e-02  Valid Loss=9.3643e-02\n",
      "\n",
      "Best model from epoch 199 with validation loss: 8.7599e-02\n",
      "Pruned Model NMSE at sparsity 0.6: -35.2588298605587 dB\n",
      "Epoch  10/200  Loss=1.9210e+00  Valid Loss=1.9447e+00\n",
      "Epoch  10/200  Loss=1.9210e+00  Valid Loss=1.9447e+00\n",
      "Epoch  20/200  Loss=9.8039e-01  Valid Loss=1.0178e+00\n",
      "Epoch  20/200  Loss=9.8039e-01  Valid Loss=1.0178e+00\n",
      "Epoch  30/200  Loss=6.7891e-01  Valid Loss=7.2909e-01\n",
      "Epoch  30/200  Loss=6.7891e-01  Valid Loss=7.2909e-01\n",
      "Epoch  40/200  Loss=5.1099e-01  Valid Loss=5.6973e-01\n",
      "Epoch  40/200  Loss=5.1099e-01  Valid Loss=5.6973e-01\n",
      "Epoch  50/200  Loss=4.0556e-01  Valid Loss=4.5596e-01\n",
      "Epoch  50/200  Loss=4.0556e-01  Valid Loss=4.5596e-01\n",
      "Epoch  60/200  Loss=3.3175e-01  Valid Loss=3.8218e-01\n",
      "Epoch  60/200  Loss=3.3175e-01  Valid Loss=3.8218e-01\n",
      "Epoch  70/200  Loss=2.8747e-01  Valid Loss=3.3468e-01\n",
      "Epoch  70/200  Loss=2.8747e-01  Valid Loss=3.3468e-01\n",
      "Epoch  80/200  Loss=2.5562e-01  Valid Loss=3.0467e-01\n",
      "Epoch  80/200  Loss=2.5562e-01  Valid Loss=3.0467e-01\n",
      "Epoch  90/200  Loss=2.3853e-01  Valid Loss=2.8347e-01\n",
      "Epoch  90/200  Loss=2.3853e-01  Valid Loss=2.8347e-01\n",
      "Epoch 100/200  Loss=2.2356e-01  Valid Loss=2.6886e-01\n",
      "Epoch 100/200  Loss=2.2356e-01  Valid Loss=2.6886e-01\n",
      "Epoch 110/200  Loss=2.1597e-01  Valid Loss=2.5387e-01\n",
      "Epoch 110/200  Loss=2.1597e-01  Valid Loss=2.5387e-01\n",
      "Epoch 120/200  Loss=2.0758e-01  Valid Loss=2.4262e-01\n",
      "Epoch 120/200  Loss=2.0758e-01  Valid Loss=2.4262e-01\n",
      "Epoch 130/200  Loss=2.0317e-01  Valid Loss=2.3724e-01\n",
      "Epoch 130/200  Loss=2.0317e-01  Valid Loss=2.3724e-01\n",
      "Epoch 140/200  Loss=1.9979e-01  Valid Loss=2.2859e-01\n",
      "Epoch 140/200  Loss=1.9979e-01  Valid Loss=2.2859e-01\n",
      "Epoch 150/200  Loss=1.9528e-01  Valid Loss=2.2607e-01\n",
      "Epoch 150/200  Loss=1.9528e-01  Valid Loss=2.2607e-01\n",
      "Epoch 160/200  Loss=1.9469e-01  Valid Loss=2.2148e-01\n",
      "Epoch 160/200  Loss=1.9469e-01  Valid Loss=2.2148e-01\n",
      "Epoch 170/200  Loss=1.9137e-01  Valid Loss=2.1860e-01\n",
      "Epoch 170/200  Loss=1.9137e-01  Valid Loss=2.1860e-01\n",
      "Epoch 180/200  Loss=1.8944e-01  Valid Loss=2.1630e-01\n",
      "Epoch 180/200  Loss=1.8944e-01  Valid Loss=2.1630e-01\n",
      "Epoch 190/200  Loss=1.8739e-01  Valid Loss=2.1613e-01\n",
      "Epoch 190/200  Loss=1.8739e-01  Valid Loss=2.1613e-01\n",
      "Epoch 200/200  Loss=1.8486e-01  Valid Loss=2.1327e-01\n",
      "\n",
      "Best model from epoch 198 with validation loss: 2.1117e-01\n",
      "Pruned Model NMSE at sparsity 0.8: -31.34112531104536 dB\n",
      "Epoch 200/200  Loss=1.8486e-01  Valid Loss=2.1327e-01\n",
      "\n",
      "Best model from epoch 198 with validation loss: 2.1117e-01\n",
      "Pruned Model NMSE at sparsity 0.8: -31.34112531104536 dB\n"
     ]
    }
   ],
   "source": [
    "for sparisty in [0.2, 0.4, 0.6, 0.8]:\n",
    "    pruned_model = nn_model.prune_model(prune_amount=sparisty)\n",
    "    nn_model_pruned = NN(pruned_model)\n",
    "    train_loader_pruned = nn_model_pruned.build_dataloaders(model_xfc, model_training_expected_output)\n",
    "    valid_loader_pruned = nn_model_pruned.build_dataloaders(valid_xfc, model_valid_expected_output)\n",
    "    train_losses_pruned, valid_losses_pruned, best_epoch_pruned = nn_model_pruned.get_best_model(train_loader_pruned, valid_loader_pruned, num_epochs=200)\n",
    "    print(f\"Pruned Model NMSE at sparsity {sparisty}: {nn_model_pruned.calculate_nmse(model_xfc, model_training_expected_output)} dB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
